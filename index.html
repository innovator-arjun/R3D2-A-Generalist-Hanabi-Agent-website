<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=" First MARL agent that can play different Hanabi settings and cooperate with other algorithmic agents">
  <meta name="keywords" content="Multi-Agent Reinforcement Learning (MARL), Cooperative game, Multi Agent Text-based game">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>R3D2: A Generalist Hanabi Agent  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Generalist Hanabi Agent</h1>
          <p style="font-size: 24px; font-weight: bold; color: #4CAF50;">
          ICLR 2025 Accepted
          </p>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.arjunvaithilingamsudhakar.com/">Arjun Vaithilingam Sudhakar</a><sup>* 1,2,3</sup>,</span>
            <span class="author-block">
              <a href="https://hnekoeiq.github.io/">Hadi Nekoei</a><sup>* 1,2,4</sup>,</span>
            <span class="author-block">
              <a href="https://mathieu-reymond.github.io/">Mathieu Reymond</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/umich.edu/janarthanan-rajendran/">Janarthanan Rajendran</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/miaoliuhome">Miao Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://sarathchandar.in/">Sarath Chandar</a><sup>1,2,3,7</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Chandar Research Lab</span>
            <span class="author-block"><sup>2</sup>Mila-Quebec AI Institute</span>
            <span class="author-block"><sup>3</sup>Polytechnique Montreal</span>
            <span class="author-block"><sup>4</sup>Universite de Montreal</span>
            <span class="author-block"><sup>5</sup>IBM Research</span>
            <span class="author-block"><sup>6</sup>Dalhousie University</span>
            <span class="author-block"><sup>7</sup>Canada CIFAR AI Chair</span>

          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=pCj2sLNoJq"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.14555"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/@chandar-lab9459"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent/tree/dev_r3d2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Checkpoints</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item item-fullbody">
        <figure class="image">
          <img src="./static/images/r3d2_high.png" alt="Fullbody Preview">
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p><strong>TL;DR:</strong> We present the first agent capable of both playing all Hanabi settings simultaneously and generalizing <em>zero-shot</em> to novel partners and game configurations. Achieving this through <strong>self-play</strong>, without complex MARL methods, demonstrates a task-agnostic approach to generalization that we believe is valuable to the MARL community.</p>  

          <p>Real-world multi-agent environments require agents that can adapt to <strong>dynamic settings</strong> and collaborate with diverse partners seamlessly, and <strong>R3D2</strong> is a path towards that goal.</p>

          <p>
            <p><strong>Traditional Multi-Agent Reinforcement Learning (MARL) Limitations:</strong></p>  
            <p>MARL systems develop cooperative strategies through repeated interactions. However, they struggle to generalize beyond their training setting and fail to collaborate effectively with unfamiliar agents.</p>  

            <p><strong>The Hanabi Benchmark Challenge:</strong></p>  
            <p>Hanabi, a popular 2-to-5 player cooperative card game, requires complex reasoning and precise assistance. Current MARL agents can only play in fixed game settings (e.g., 2-player) and with the same algorithmic agents, unlike humans who adapt dynamically to new partners.</p>  

            <p><strong>Introducing R3D2:</strong></p>  
            <ul>  
              <li><strong>Recurrent Replay Relevance Distributed DQN (R3D2)</strong> is a generalist agent designed to overcome these limitations.</li>  
              <li>We reformulate Hanabi using text-based inputs, leveraging language for improved transfer learning.</li>  
              <li>R3D2  is an improved version of R2D2 that can handle dynamic observation and action spaces.</li>  
              <li>R3D2 can do zero-shot coordination with novel partners by only relying on self-play training.</li>  
            </ul>  

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="content">
          <h2 class="title is-3">Hanabi Template - Text</h2>
          <p><strong>Our first contribution:</strong> We frame Hanabi as a text-based game, inspired by the success of language models like GPT in transfer learning.This textual representation includes all necessary game information: <u>Life and clue tokens, Visible hands, Discarded cards, Hints. </u></p> 

            <figure class="image">
              <img src="./static/images/hanabi_archi.jpg" alt="Fullbody Preview" style="width: 45%; height: auto;">
            </figure>


          <h2 class="title is-3">LLM results on Hanabi-Text:</h2>
          <p>We evaluated the <strong>state-of-the-art LLMs</strong> by prompting them on some expert Hanabi trajectories to assess if we could develop a <strong>generalist Hanabi agent</strong> without any online reinforcement learning.</p>  

          <p>Our results demonstrate that <strong>current LLMs are still far from solving text-based Hanabi</strong>. For instance, the <code>o3-mini</code> model achieved only <strong>12 out of 25</strong> with a reasonable amount of prompting effort.</p>


            <figure class="image">
              <img src="./static/images/llm_results.png" alt="Fullbody Preview" style="width: 80%; height: auto;">
            </figure>

        </div>
        <!-- Interpolating. -->
        <h3 class="title is-4">Policy Transfer - Zeroshot setting:</h3>
        <div class="content has-text-justified">
          <p><strong>Background:</strong></p>  
          <p>In <strong>multi-agent Hanabi</strong>, increasing the number of players introduces <strong>greater complexity</strong>, making strategic coordination more difficult. <strong>R3D2</strong> observes and interacts with its environment through <em>natural language</em>, which helps transfer learned behavior to other tasks. It can also be <strong>deployed in different game settings</strong> without any modification to the architecture.</p>

          <p><strong>Key Observations:</strong></p>  
          <ul>  
            <li>As the number of players increases, overall performance decreases due to increased strategy complexity.</li>  
            <li><strong>R3D2-M</strong> learns competitive strategies across settings and effectively transfers knowledge, unlike <code>R2D2-text</code>.</li>  
            <li>Despite training via <strong>self-play</strong>, <strong>R3D2</strong> maintains high cross-play performance, adapting well to new partners.</li>  
            <li>Dynamic action-space handling is crucial for learning generalizable policies, beyond just textual observations.</li>  
          </ul>

          
        </div>
        <figure class="image">
          <img src="./static/images/results.png" alt="Fullbody Preview">
        </figure>

        <br/>
        <!--/ Interpolating. -->
        <h3 class="title is-4">Zero-shot coordination matrix:</h3>
        <!-- Results. -->
        <div class="content has-text-justified">
        <p><strong>Intra-cross-play (Intra-XP): </strong> Enables agents within the same system to collaborate using shared strategies.</p>  

        <p><strong>Inter-cross-play (Inter-XP): </strong> Facilitates interactions between distinct agent groups or external partners, allowing for more diverse coordination.</p>  

        <p><strong>Self-play (SP): </strong> Trains agent by having it compete against its own evolving iterations, enabling continuous improvement.</p>

        <p><strong>R3D2's Robustness in Cross-Play:</strong></p>
        <p>We compared <strong>R3D2â€™s policies</strong> in cross-play settings with other baseline algorithms, evaluating performance across multiple seeds. The results highlight <strong>R3D2's robustness</strong>, especially in <strong>inter-XP</strong>, where it outperforms <code>R2D2</code> and <code>R2D2-Other Play (OP)</code>. This suggests that R3D2 learns more general strategies, making it more adaptable to new agents and game settings.</p>  

        <p>In a <strong>2-player setting</strong>, we observed that <code>R3D2-S</code> outperforms <code>IQL</code> and <code>OP</code> in all metrics and achieves a performance close to Off-Belief Learning<code>OBL</code> in both <strong>SP</strong> and <strong>Intra-XP</strong>, while surpassing <code>OBL</code> in <strong>inter-XP</strong>. Importantly, this is achieved solely through <strong>self-play pre-training</strong> without any ZSC-specific method, demonstrating that <strong>R3D2 can effectively coordinate with other partners.</strong></p>


        
        <!--/ Results. -->


       <figure class="image">
          <img src="./static/images/zero.png" alt="Fullbody Preview">
        </figure>

          <h2 class="title is-3">Video</h2>
          <div class="publication-video">

          <iframe src="https://www.youtube.com/embed/2PUEib21nFc?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

          </div>


        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
      sudhakar2025a,
      title={A Generalist Hanabi Agent},
      author={Arjun V Sudhakar and Hadi Nekoei and Mathieu Reymond and Miao Liu and Janarthanan Rajendran and Sarath Chandar},
      booktitle={The Thirteenth International Conference on Learning Representations},
      year={2025},
      url={https://openreview.net/forum?id=pCj2sLNoJq}
      }</code></pre>
  </div>
</section>

<!-- 
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
